= Using Jaeger in Service Mesh / Istio
:imagesdir: images
:toc:

[discrete]
=== General Instructions

. Clone the git repository
+
[source, bash]
----
git clone https://github.com/csantanapr/learning-distrubing-tracing-101.git
----

. Change to the lab directory
+
[source, bash]
----
cd lab-jaeger-istio
----

== Understanding Jaeger, Service Mesh, Kiali

* Read the OpenShift Documentation for:
** https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_arch/ossm-jaeger.html[Understanding Jaeger]
** https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_arch/understanding-ossm.html[Understanding Service Mesh]
** https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_arch/ossm-kiali.html[Understanding Kiali]

== Installing Service Mesh (Istio) Operator

As part of the Service Mesh installation, the Jaeger operator is also installed, in addition Prometheus, Grafana, and Kiali are also configured and installed.

* OpenShift 4, You can use the CodeReady Containers for local development cluster https://cloud.redhat.com/openshift/install/crc/installer-provisioned
** https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_install/preparing-ossm-installation.html[Preparing to install Red Hat OpenShift Service Mesh]
** https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_install/installing-ossm.html[Installing Red Hat OpenShift Service Mesh]

[IMPORTANT]
Reducing resources when using local dev crc
====
When creating the ServiceMeshControlPlane edit the yaml for the pilot to specified a lower memory request, the default is 2Gi too high for the crc VM, the pilot section should look this:
[source, yaml]
----
    pilot:
      autoscaleEnabled: false
      traceSampling: 100
      resources:
          requests:
            cpu: 100m
            memory: 128Mi
----

====

== Verify Service Mesh installation

. Verify that istio components are installed in the namespace `istio-system`
+
[source, bash]
----
oc get pods -n istio-system
NAME                                    READY   STATUS    RESTARTS   AGE
grafana-cf5ccd86-dtcl8                  2/2     Running   0          2m9s
istio-citadel-7cb44f4bb-hlzdc           1/1     Running   0          5m42s
istio-egressgateway-58f4b474c4-mlj8v    1/1     Running   0          2m50s
istio-galley-75599dbc67-ch2tv           1/1     Running   0          4m46s
istio-ingressgateway-769c96c46f-dlrv8   1/1     Running   0          2m50s
istio-pilot-7bd6bc45cf-rcfdl            2/2     Running   0          3m29s
istio-policy-56476c984b-44sw5           2/2     Running   0          4m23s
istio-sidecar-injector-55c7bf57-x25rr   1/1     Running   0          2m34s
istio-telemetry-d5bbd7d7b-pqv8r         2/2     Running   0          4m23s
jaeger-5d9dfdfb67-658t5                 2/2     Running   0          4m51s
kiali-597c76cc67-5qs2d                  1/1     Running   0          52s
prometheus-685bdbdc45-rljkq             2/2     Running   0          5m23s
----

. Verify services in the namespace `istio-system`
+
[source, bash]
----
oc get services -n istio-system
NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                  AGE
grafana                     ClusterIP   172.30.247.177   <none>        3000/TCP                                 16m
istio-citadel               ClusterIP   172.30.95.16     <none>        8060/TCP,15014/TCP                       20m
istio-egressgateway         ClusterIP   172.30.255.61    <none>        80/TCP,443/TCP,15443/TCP                 17m
istio-galley                ClusterIP   172.30.91.96     <none>        443/TCP,15014/TCP,9901/TCP               19m
istio-ingressgateway        ClusterIP   172.30.26.182    <none>        15020/TCP,80/TCP,443/TCP,15443/TCP       17m
istio-pilot                 ClusterIP   172.30.149.199   <none>        15010/TCP,15011/TCP,8080/TCP,15014/TCP   17m
istio-policy                ClusterIP   172.30.22.227    <none>        9091/TCP,15004/TCP,15014/TCP             18m
istio-sidecar-injector      ClusterIP   172.30.145.35    <none>        443/TCP                                  16m
istio-telemetry             ClusterIP   172.30.254.60    <none>        9091/TCP,15004/TCP,15014/TCP,42422/TCP   18m
jaeger-agent                ClusterIP   None             <none>        5775/TCP,5778/TCP,6831/TCP,6832/TCP      19m
jaeger-collector            ClusterIP   172.30.157.38    <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP   19m
jaeger-collector-headless   ClusterIP   None             <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP   19m
jaeger-query                ClusterIP   172.30.15.164    <none>        443/TCP                                  19m
kiali                       ClusterIP   172.30.124.111   <none>        20001/TCP                                15m
prometheus                  ClusterIP   172.30.212.70    <none>        9090/TCP                                 19m
zipkin                      ClusterIP   172.30.130.27    <none>        9411/TCP                                 19m
----

. Verify that the ServiceMeshMemberRoll includes the target namespace for example `default` as one of the `MEMBERS`
+
[source, bash]
----
oc get ServiceMeshMemberRoll -n istio-system
NAME      MEMBERS
default   [default bookinfo]
----

. Verify routes in to the different UI dashboards for Jaeger, Grafana, and Kiali
+
[source, bash]
----
oc get route -n istio-system
NAME                   HOST/PORT                                            PATH   SERVICES               PORT    TERMINATION   WILDCARD
grafana                grafana-istio-system.apps-crc.testing                       grafana                <all>   reencrypt     None
istio-ingressgateway   istio-ingressgateway-istio-system.apps-crc.testing          istio-ingressgateway   8080                  None
jaeger                 jaeger-istio-system.apps-crc.testing                        jaeger-query           <all>   reencrypt     None
kiali                  kiali-istio-system.apps-crc.testing                         kiali                  <all>   reencrypt     None
prometheus             prometheus-istio-system.apps-crc.testing                    prometheus             <all>   reencrypt     None
----
+
* Open the different UIs in the browser using the route's values for HOST/PORT
** Jaeger: https://jaeger-istio-system.apps-crc.testing
** Grafana: https://grafana-istio-system.apps-crc.testing
** Kiali: https://kiali-istio-system.apps-crc.testing

== Deploy the Application

. Deploy the services `service-a` and `service-b`
+
Use the file `jaeger-nodejs.yaml` for Node.js or the file `jaeger-java.yaml` for Java
+
Here is an example using Node.js services:
+
[source, bash]
----
oc apply -f istio-java.yaml -n default
----
Let's look at the file content on how the services are defined to be deploy into OpenShift cluster:
+
[source, yaml]
----
---
---
apiVersion: v1
kind: Service
metadata:
  name: service-a
  labels:
    app: service-a
spec:
  ports:
    - port: 8080
      name: http
  selector:
    app: service-a
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-a
  labels:
    app: service-a
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-a
  template:
    metadata:
      labels:
        app: service-a
        version: v1
      annotations:
        sidecar.istio.io/inject: "true"
    spec:
      containers:
        - name: app
          image: csantanapr/service-a-java
          env:
            - name: JAEGER_ENDPOINT
              value: http://jaeger-collector.istio-system.svc:14268/api/traces
            - name: JAEGER_PROPAGATION
              value: b3
            - name: SERVICE_FORMATTER
              value: service-b
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: service-b
  labels:
    app: service-b
spec:
  ports:
    - port: 8081
      name: http
  selector:
    app: service-b
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-b
  labels:
    app: service-b
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-b
  template:
    metadata:
      labels:
        app: service-b
        version: v1
    annotations:
      sidecar.istio.io/inject: "true"
    spec:
      containers:
        - name: app
          image: csantanapr/service-b-java
          env:
            - name: JAEGER_ENDPOINT
              value: http://jaeger-collector.istio-system.svc:14268/api/traces
            - name: JAEGER_PROPAGATION
              value: b3
          imagePullPolicy: Always
          ports:
            - containerPort: 8081
----
+
In the yaml deployment manifest there are few items to point out:
+
* **Ports**
** The port for the container is specified in the service and the container in the deployment, for example `service-a` with port `8080` and `service-b` with port `8081`
* **Environment Variables**
** The variable `JAEGER_ENDPOINT` is specified to indicate to the Jaeger client library to send the traces using http to the jaeger collector service `http://jaeger-collector.istio-system.svc:14268/api/traces` that is deployed on the namespace `istio-system`. 
** The variable `SERVICE_FORMATTER` used by `service-a` to indicate the hostname of `service-b` that will use to format the hello message.
* Istio has certain https://istio.io/docs/setup/additional-setup/requirements/[specific requirements] the ones we used in our yaml manifest are the following
** *Named service ports* 
*** The service port name value start with `http`
** **Deployment with app and version labels**
*** THe Pod template should have a unique `app` label, and a `version`
NOTE: We are setting the JAEGER_PROPAGATION=b3 parameter and including the jaeger-zipkin artifact. This is necessary because the Envoy proxy does not recognize Jaeger's default on-the-wire representation of the trace context, but it does recognize Zipkin's B3 headers. This configuration instructs the Jaeger tracer to use B3 headers instead of its default ones.
. Verify services are deployed and running:
+
[source, bash]
----
oc get all -l app=service-a -n default
oc get all -l app=service-b -n default
NAME                             READY     STATUS    RESTARTS   AGE
pod/service-a-74cd5c6496-nvllm   2/2       Running   0          6m7s
pod/service-b-674f96464b-hbmg7   2/2       Running   0          6m44s

NAME                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/service-a   ClusterIP   172.30.44.43   <none>        8080/TCP   6m7s
service/service-b   ClusterIP   172.30.115.93   <none>        8081/TCP   6m45s

NAME                        READY     UP-TO-DATE   AVAILABLE   AGE
deployment.apps/service-a   1/1       1            1           6m7s
deployment.apps/service-b   1/1       1            1           6m44s
----

. Expose the service `service-a` with a route
+
[source, bash]
----
oc create route edge --service=service-a -n default
----

. Get the hostname for the route:
+
[source, bash]
----
oc get route service-a -n default
NAME        HOST/PORT                            PATH   SERVICES    PORT   TERMINATION   WILDCARD
service-a   service-a-default.apps-crc.testing          service-a   http   edge          None
----

. Use curl or open browser with the endpoint URL using the HOST/PORT of the route
+
[source, bash]
----
curl -k https://service-a-default.apps-crc.testing/sayHello/Carlos
Hello from service-b Carlos!
----
+
From the result you can see that `service-a` called `service-b` and replied back.

. In the Jaeger UI select service-a and click **Find Traces**
+
image::ocp-jaeger-traces.png[]

. Click on one of the traces and expand the spans in the trace
+
image::ocp-jaeger-spans.png[]

Check one of the labs xref:lab-jaeger-nodejs.adoc[Lab Jaeger - Node.js] or xref:lab-jaeger-java.adoc[Lab Jaeger - Java] for a more in depth lab for Opentracing with Jaeger.





